https://disk.yandex.ru/d/wLpuky3FpBQ9qw

#df['elapased_months'] = df.today.dt.to_period('M').view(dtype='int64') - \
#                        df.date.dt.to_period('M').view(dtype='int64')

def q_days(df, date_in, n=3):
  #for example date_in = '2016-12-01', n = 3 (months)

  l_dates = pd.date_range(pd.to_datetime(date_in) - np.timedelta64(n-1, 'M'), periods=n, freq='m')

  d_s = sum([int(i.strftime('%d')) for i in l_dates])

  l_d = [i.strftime('%Y%m') for i in l_dates]

  df.loc[df.month_id.isin(l_d), 'd'].sum()

======================================================

https://towardsdatascience.com/simple-gps-data-visualization-using-python-and-open-street-maps-50f992e9b676

import numpy as np
from PIL import Image, ImageDraw
import PIL.ImageOps 
from matplotlib import cm
"""
a = np.ones((256, 256))

im = Image.fromarray(np.uint8(cm.gist_earth(a)*255))

display(im)
#im.save('test.png')
im.size
"""

"""
a = np.ones((256, 256))

PIL_image = Image.fromarray(np.uint8(a)*255).convert('RGB')

display(PIL_image)
PIL_image.size
"""

#image = Image.open('test.png')

# create an image
image = Image.new("RGB", (286, 286), (255, 255, 255))

draw = ImageDraw.Draw(image)
x, y, r = image.size[0]/2, image.size[1]/2, 5
draw.ellipse((x-r, y-r, x+r, y+r), fill=(255,0,0,0))

cropped_img = image.crop((15, 15, image.size[0]-15, image.size[1]-15))

#display(image)
display(PIL.ImageOps.invert(cropped_img))
#image.save('test_2.png')
print("image.size: %s" %str(image.size))
print("cropped_img.size: %s" %str(cropped_img.size))

################################################
import numpy as np
from sklearn.cluster import KMeans
import random
import pandas as pd
import string

def id_generator(size=6, chars=string.ascii_uppercase + string.digits):
    return ''.join(random.choice(chars) for _ in range(size))

def sum_generator(size=10, band_coef=range(8,13)):
    return [np.random.rand(1,random.choice(band_coef))[0].tolist()
                      for i in range(size)]


n = 100
n_clust = 4 
x = np.zeros([n, n])
x[np.triu_indices(n, 1)] = np.random.randint(0, 100, (1,(n*n - n)/2))[0]
x += x.T

g_struct = {}
g_struct['ID'] = range(n)
g_struct['GR'] = range(n_clust)
g_struct['SZ'] = np.random.randint(200, 1000, (1,n_clust))[0]
g_struct['Sum'] = sum_generator(size=n)

kmeans = KMeans(n_clusters=4, random_state=0).fit(x)

indx = []
for i in np.unique(kmeans.labels_):
    indx.append((kmeans.labels_ == i).nonzero()[0].tolist())
    print("{}: {}".format(len(indx[-1]), indx[-1]))
#
g_struct['Nets'] = indx

new_samp = [1, 501, 4]

c_seas = []
for i in range(12):
    tmp_df = pd.DataFrame(np.array(g_struct['Sum'])[g_struct['Nets'][new_samp[0]]].tolist())
    c_seas.append(tmp_df.loc[:, 11].dropna().mean())
c_seas = c_seas/np.sum(c_seas)
print(c_seas)
print(c_seas[new_samp[2]])

tmp_df = pd.DataFrame(np.array(g_struct['Sum'])[g_struct['Nets'][new_samp[0]]].tolist())
print(tmp_df)

############################################################################################

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def anomal_filt(sig, w=3, s=1, d1=0.05, d2=1-0.05, q_s1=3, q_s2=3):
    res = []
    i = 0
    while(i < len(sig)-w):
        z1 = np.abs(sig[i]-sig[i+1])
        z2 = np.abs(sig[i]-sig[i+2])
        if((z1<d1*sig[i]) & (z2<d1*sig[i])):
            res = sig[:i+q_s1]
            #print(z1,z2)
            #print('plato')
            i = len(sig)
        elif((sig[i+1] < d2*sig[i]) & (sig[i+2] < d2*sig[i+1])):
            res = sig[:i+q_s2]
            #print(sig[:i+3])
            #print('low')
            i = len(sig)
        else:
            i = i + 1
    if(len(res)==0):
       res = sig
    return res

def anomal_filt2(sig, w=3, s=1, d1=0.05, d2=1-0.05):
    res = []
    i = 0
    while(i < len(sig)-w):
        z1 = np.abs(sig[i]-sig[i+1])
        z2 = np.abs(sig[i]-sig[i+2])
        if((z1<d1*sig[i]) & (z2<d1*sig[i])):
            #res.append(np.nan)
            #res.append(np.nan)
            #i += 3
            i = len(sig)
        elif((sig[i+1] < d2*sig[i]) & (sig[i+2] < d2*sig[i+1])):
            res.append(np.nan)
            res.append(np.nan)
            i += 3
        else:
            res.append(sig[i])
            i += 1
        if((i > len(sig)-w) | (i == len(sig)-w)):
            res.extend(sig[i:])
    return res

def wr_f(path_f='TMP.txt'):
    with open(path_f, 'w') as of:
        for j in noise:
            of.write("\t".join([str(i) for i in j]))
            of.write("\n")

def re_f(path_f='TMP.txt'):
    res = []
    with open(path_f, 'r') as of:
        for i in of.read().split('\n'):
            res.append([float(j) for j in i.split('\t')])
    return res

def main():
    # Generate signal
    #q_sig = 100
    #noise = np.ones((q_sig,1)) * np.linspace(5,17,12) + \
    #        np.random.normal(0,5,(q_sig,12))
    #
    noise = np.array(re_f())
    rng = np.random.RandomState(42)
    noise = np.abs(rng.normal(10,50,(noise.shape))) + noise
    #
    plt.subplot(311)
    res = []
    a = pd.DataFrame()
    for i in range(len(noise)):
        #res.append(anomal_filt(noise[i], q_s1=0, q_s2=0))
        res.append(anomal_filt2(noise[i], d1=0.2, d2=1-0.05))
        plt.plot(res[-1])
        a = pd.concat([a, pd.DataFrame({'s'+str(i): res[-1]}, index=range(len(res[-1])))], axis=1)
        print(res[-1].__len__())
    print('ok')
    out = []
    out2 = []
    for i in range(a.shape[0]):
        tmp = a.loc[i,:].dropna()
        out.append(tmp)
        out2.append(np.median(tmp))
    plt.subplot(312)
    plt.bar(range(len(out)), [len(i) for i in out])
    print('ok')
    plt.subplot(313)
    #plt.boxplot(out)
    plt.plot(out2)
    plt.show()

if __name__ == '__main__':
    main()
    
############################################################################################

# -*- coding: utf-8 -*-
from random import random,randint,choice
from copy import deepcopy
from math import log

class fwrapper:
    def __init__(self,function,childcount,name):
        self.function=function
        self.childcount=childcount
        self.name=name

class node:
    def __init__(self,fw,children):
        self.function=fw.function
        self.name=fw.name
        self.children=children
    def evaluate(self,inp):
        results=[n.evaluate(inp) for n in self.children]
        return self.function(results)
    def display(self,indent=0):
        print (' '*indent)+self.name
        for c in self.children:
            c.display(indent+1)

class paramnode:
    def __init__(self,idx):
        self.idx=idx
    def evaluate(self,inp):
        return inp[self.idx]
    def display(self,indent=0):
        print '%sp%d' % (' '*indent,self.idx)

class constnode:
    def __init__(self,v):
        self.v=v
    def evaluate(self,inp):
        return self.v
    def display(self,indent=0):
        print '%s%d' % (' '*indent,self.v)

addw=fwrapper(lambda l:l[0]+l[1],2,'add')
subw=fwrapper(lambda l:l[0]-l[1],2,'subtract')
mulw=fwrapper(lambda l:l[0]*l[1],2,'multiply')

def iffunc(l):
    if l[0]>0: return l[1]
    else: return l[2]
ifw=fwrapper(iffunc,3,'if')

def isgreater(l):
    if l[0]>l[1]: return 1
    else: return 0
gtw=fwrapper(isgreater,2,'isgreater')
flist=[addw,mulw,ifw,gtw,subw]

def exampletree( ):
    return node(ifw,[
                node(gtw,[paramnode(0),constnode(3)]),
                node(addw,[paramnode(1),constnode(5)]),
                node(subw,[paramnode(1),constnode(2)]),
            ]
        )

def makerandomtree(pc,maxdepth=4,fpr=0.5,ppr=0.6):
    if random( )<fpr and maxdepth>0:
        f=choice(flist)
        children=[makerandomtree(pc,maxdepth-1,fpr,ppr)
            for i in range(f.childcount)]
        return node(f,children)
    elif random( )<ppr:
        return paramnode(randint(0,pc-1))
    else:
        return constnode(randint(0,10))

def hiddenfunction(x,y):
    return x**2+2*y+3*x+5

def buildhiddenset( ):
    rows=[]
    for i in range(200):
        x=randint(0,40)
        y=randint(0,40)
        rows.append([x,y,hiddenfunction(x,y)])
    return rows

def scorefunction(tree,s):
    dif=0
    for data in s:
        v=tree.evaluate([data[0],data[1]])
        dif+=abs(v-data[2])
    return dif

def mutate(t,pc,probchange=0.1):
    if random( )<probchange:
        return makerandomtree(pc)
    else:
        result=deepcopy(t)
        if isinstance(t,node):
            result.children=[mutate(c,pc,probchange) for c in t.children]
        return result

def crossover(t1,t2,probswap=0.7,top=1):
    if random( )<probswap and not top:
        return deepcopy(t2)
    else:
        result=deepcopy(t1)
        if isinstance(t1,node) and isinstance(t2,node):
            result.children=[crossover(c,choice(t2.children),probswap,0)
                for c in t1.children]
        return result

def evolve(pc,popsize,rankfunction,maxgen=500,
           mutationrate=0.1,breedingrate=0.4,pexp=0.7,pnew=0.05):
    # Возвращает случайное число, отдавая предпочтение более маленьким числам
    # Чем меньше значение pexp, тем больше будет доля маленьких чисел
    def selectindex( ):
        return int(log(random( ))/log(pexp))
    # Создаем случайную исходную популяцию
    population=[makerandomtree(pc) for i in range(popsize)]
    for i in range(maxgen):
        scores=rankfunction(population)
        print scores[0][0]
        if scores[0][0]==0: break
        # Две наилучшие особи отбираются всегда
        newpop=[scores[0][1],scores[1][1]]
        # Строим следующее поколение
        while len(newpop)<popsize:
            if random( )>pnew:
                newpop.append(mutate(
                crossover(scores[selectindex( )][1],
                scores[selectindex( )][1],
                probswap=breedingrate),
                pc,probchange=mutationrate))
            else:
                # Добавляем случайный узел для внесения неопределенности
                newpop.append(makerandomtree(pc))
        population=newpop
    scores[0][1].display( )
    return scores[0][1]

def getrankfunction(dataset):
    def rankfunction(population):
        scores=[(scorefunction(t,dataset),t) for t in population]
        scores.sort( )
        return scores
    return rankfunction


import time
import random

def calculatePrimeFactors(n):
    primfac = []
    d = 2
    while d*d <= n:
        while (n % d) == 0:
            primfac.append(d) # supposing you want multiple factors repeated
            n //= d
        d += 1
    if n > 1:
        primfac.append(n)
    return primfac

def main():
    print("Starting number crunching")
    t0 = time.time()

    for i in range(10000):
        rand = random.randint(20000, 10**8)
        t = calculatePrimeFactors(rand)

    t1 = time.time()
    totalTime = t1 - t0
    print("Execution Time: {}".format(totalTime))

if __name__ == '__main__':
    main()
    
    
 import time
import random
from multiprocessing import Process
# This does all of our prime factorization on a given number 'n'
def calculatePrimeFactors(n):
    primfac = []
    d = 2
    while d*d <= n:
        while (n % d) == 0:
            primfac.append(d) # supposing you want multiple factors repeated
            n //= d
        d += 1
    if n > 1:
        primfac.append(n)
    return primfac
# We split our workload from one batch of 10,000 calculations
# into 10 batches of 1,000 calculations
def executeProc():
    for i in range(10000):
        rand = random.randint(20000, 100000000)
        print(calculatePrimeFactors(rand))
def main():
    print("Starting number crunching")
    t0 = time.time()
    procs = []
    # Here we create our processes and kick them off
    for i in range(10):
        proc = Process(target=executeProc, args=())
        procs.append(proc)
        proc.start()
    # Again we use the .join() method in order to wait for
    # execution to finish for all of our processes
    for proc in procs:
        proc.join()
    t1 = time.time()
    totalTime = t1 - t0
    # we print out the total execution time for our 10
    # procs.
    print("Execution Time: {}".format(totalTime))
if __name__ == '__main__':
    main()   










import os
import requests
from tqdm import tqdm
from google.colab import files

url = ""
response = requests.get(url, stream=True)

with open("test.mp4", "wb") as handle:
    for data in tqdm(response.iter_content()):
        handle.write(data)

print(os.listdir(os.getcwd()))
files.download('/content/test.mp4')
===============================================================================
===============================================================================
===============================================================================
# -*- coding: utf-8 -*-
#https://stackoverflow.com/questions/42172926/pool-map-list-index-out-of-range-python
#http://qaru.site/questions/1139485/python-multiprocessing-poolmap-raises-indexerror
#http://qaru.site/questions/1139487/debugging-errors-in-python-multiprocessing
import time
import pandas as pd
import multiprocessing as mp,os
import pickle

count = 0

def t_process(line):
    global count
    t_line = line[:-1].split(';')
    print(t_line[-1])
    if (t_line[-1]==1):
        count = count + 1
        print(count)

def process_wrapper(chunkStart, chunkSize, item):
    with open("") as f:
        f.seek(chunkStart)
        lines = f.read(chunkSize).splitlines()
        out = []
        for line in lines:
            t_l = line[:-1].split(';')
            if (t_l[1] == item):
                out.append( [int(t_l[2]), float(t_l[6].replace(',','.'))] )
    return out

def chunkify(fname,size=1024*1024*128):
    fileEnd = os.path.getsize(fname)
    with open(fname,'r') as f:
        chunkEnd = f.tell()
        while True:
            chunkStart = chunkEnd
            f.seek(size, 1)
            f.readline()
            chunkEnd = f.tell()
            yield chunkStart, chunkEnd - chunkStart
            if chunkEnd > fileEnd:
                break

if __name__ == '__main__':
    cores = 2
    #init objects
    #pool = mp.Pool(cores)
    #jobs = []

    with open("") as of:
        tmp_r = of.read().split('\n')
    struc = {}
    for item1 in tmp_r:
        print(item1)
        pool = mp.Pool(cores)
        jobs = []
        #create jobs
        for chunkStart,chunkSize in chunkify(""):
            #print(chunkStart,chunkSize)
            jobs.append( pool.apply_async(process_wrapper,(chunkStart,chunkSize,item1)) )
        print(len(jobs))
        #clean up
        pool.close()
        pool.join()
        #wait for all jobs to finish
        tmp = []
        for job in jobs:
            #time.sleep(20)
            tmp.extend( job.get())
            #tmp = pd.Series(tmp).unique().tolist()
            print(len(tmp))
        print('' + str(item1) + ' done!')
        #
        tmp_m = pd.DataFrame(tmp)
        struc[int(item1)] = {}
        for ind1 in tmp_m[0].unique():
            struc[int(item1)][ind1] = sum(tmp_m.loc[tmp_m.index[tmp_m[0]==ind1], 1])
        with open("" + "_" + str(item1) + ".txt", "w") as of:
            of.writelines([str((i,struc[int(item1)][i]))[1:-1] +\
             '\n' for i in struc[int(item1)].keys()])
    #
    file = open('filename_save.pkl', 'w')
    pickle.dump(struc, file)
    print('all done')
===============================================================================
===============================================================================
===============================================================================
# -*- coding: utf-8 -*-
import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#
def fun_proc(data):
    bs = np.percentile(data, [25,50,75], axis=0)
    h = plt.hist(data)
    inds = (h[0] > bs[-1]).nonzero()
    if (np.max(h[0][inds]) - np.min(h[0][inds]) != 0):
        return (np.max(h[1][inds]) - np.min(h[1][inds]))/\
            (np.max(h[0][inds]) - np.min(h[0][inds]))
    else: return 0

def main():
    if (len(sys.argv[1]) == 0):
        print("Error: No input a file")
    else:
        pathdir = sys.argv[1]
        out = []
        for item1 in os.listdir(pathdir):
            data = pd.read_csv(pathdir + '\\' + item1, sep=",", header=None)
            out.append( [int(item1.split("_")[-1][:-4]),fun_proc(data[1])] )
        #
        y = np.array(out)
        bs = np.percentile(y[:,1], [25,50,75], axis=0)
        inds1 = ((y[:,1] < bs[0]) | (y[:,1] == bs[0])).nonzero()[0]
        inds2 = ((y[:,1] > bs[0]) & (y[:,1] < bs[1]) | (y[:,1] == bs[1])).nonzero()[0]
        inds3 = ((y[:,1] > bs[1]) & (y[:,1] < bs[2])).nonzero()[0]
        inds4 = ((y[:,1] > bs[2]) | (y[:,1] == bs[2])).nonzero()[0]
        # add label
        labels = np.ones((len(y[:,1]),1))
        print(labels.shape)
        if (len(inds1) != 0):
            labels[inds1] = 0
        if (len(inds2) != 0):
            labels[inds2] = 1
        if (len(inds3) != 0):
            labels[inds3] = 2
        if (len(inds4) != 0):
            labels[inds4] = 3
        y = np.hstack((y,labels))
        # save result to file
        with open("out_file.txt", "w") as of:
            for i in y:
                of.writelines(str(i)[1:-1] + '\n')

if __name__ == '__main__':
    main()
===============================================================================
===============================================================================
===============================================================================

#-------------------------------------------------------------------------------
# Name:        module1
# Purpose:
#
# Author:      admin2
#
# Created:     12.11.2018
# Copyright:   (c) admin2 2018
# Licence:     <your licence>
#-------------------------------------------------------------------------------
import numpy as np
import multiprocessing as mp,os

PathFile = "input.txt"

def process_wrapper(chunkStart, chunkSize):
    with open(PathFile) as f:
        f.seek(chunkStart)
        lines = f.read(chunkSize).splitlines()
        for line in lines:
            t_l = line[-1].split(';')
            struc = fun_pars_ch(t_l[1], t_l[2], t_l[5], struc)
    return struc

def chunkify(fname,size=1024*1024*128):
    fileEnd = os.path.getsize(fname)
    with open(fname,'r') as f:
        chunkEnd = f.tell()
        while True:
            chunkStart = chunkEnd
            f.seek(size,1)
            f.readline()
            chunkEnd = f.tell()
            yield chunkStart, chunkEnd - chunkStart
            if chunkEnd > fileEnd:
                break

def fun_pars_ch(a, b, c, st):
    t1_ind = (np.array([int(i) for i in st.keys()]) == a).nonzero()[0]
    if (len(t1_ind) == 1):
        t2_ind = (np.array([int(i) for i in st[str(a)].keys()]) == b).nonzero()[0]
        if (len(t2_ind) == 1):
            st[str(a)][str(b)] = st[str(a)][str(b)] + c
        else:
            st[str(a)][str(b)] = c
    else:
        st[str(a)] = {str(b): c}
    return st

def main():
    cores = 4
    #init objects
    pool = mp.Pool(cores)
    jobs = []

    #create jobs
    for chunkStart,chunkSize in chunkify(PathFile):
        jobs.append( pool.apply_async(process_wrapper,(chunkStart,chunkSize)) )
    tmp = []
    #wait for all jobs to finish
    for job in jobs:
        tmp.extend(job.get())

    #clean up
    pool.close()

    struc_out = {}
    for i in tmp:
        for j in i.keys():
            for k in j[i].keys():
                struc_out = fun_pars_ch(i[j][k], i[j][k], i[a.keys()[0]], struc_out)
    #
    #print(struc)


if __name__ == '__main__':
    main()

test




#-------------------------------------------------------------------------------
# Name:        module1
# Purpose:
#
# Author:      admin2
#
# Created:     13.11.2018
# Copyright:   (c) admin2 2018
# Licence:     <your licence>
#-------------------------------------------------------------------------------
#https://stackoverflow.com/questions/27731458/fastest-way-to-write-large-csv-with-python
import random
import uuid
import numpy as np
import csv
import os

def main1():
    outfile = 'data.csv'
    outsize = 1024 * 1024 * 1024 # 1GB
    with open(outfile, 'ab') as csvfile:
        size = 0
        while size < outsize:
            txt = '%s,%.6f,%.6f,%i\n' % (uuid.uuid4(), random.random()*50, random.random()*50, random.randrange(1000))
            size += len(txt)
            csvfile.write(txt)

def main2():
    outfile = 'data-alt.csv'
    outsize = 1000 # MB
    chunksize = 1000
    with open(outfile, 'ab') as csvfile:
        while (os.path.getsize(outfile)//1024**2) < outsize:
            data = [[uuid.uuid4() for i in range(chunksize)],
                    np.random.random(chunksize)*50,
                    np.random.random(chunksize)*50,
                    np.random.randint(1000, size=(chunksize,))]
            csvfile.writelines(['%s,%.6f,%.6f,%i\n' % row for row in zip(*data)])

if __name__ == '__main__':
    main2()


test2


# -*- coding: utf-8 -*-
# This import registers the 3D projection, but is otherwise unused.
from mpl_toolkits.mplot3d import Axes3D 
from numpy.random import beta
import matplotlib.pyplot as plt
import scipy as sc

size=10000
x = scipy.arange(size)
y1 = beta(10, 10, size=size)*25
y2 = beta(4, 12, size=size)*25
y3 = beta(50, 12, size=size)*25


h = [h1, h2, h3]

fig = plt.figure(figsize=(20,10))

ax0 = fig.add_subplot(221)

h1 = plt.hist(y1, histtype="stepfilled",
            bins=25, alpha=0.8, density=True)
h2 = plt.hist(y2, histtype="stepfilled",
            bins=25, alpha=0.8, density=True)
h3 = plt.hist(y3, histtype="stepfilled",
            bins=25, alpha=0.8, density=True)

ax1 = fig.add_subplot(223, projection='3d')

y = []
colors = ['r', 'g', 'b']
yticks = [3, 2, 1]
for c, k, s in zip(colors, yticks, h):
    # Generate the random data for the y=k 'layer'.
    ys = s[0]
    xs = np.arange(len(ys))
    # You can provide either a single color or an array with the same length as
    # xs and ys. To demonstrate this, we color the first bar of each set cyan.
    cs = [c] * len(xs)
    #cs[0] = 'c'

    # Plot the bar graph given by xs and ys on the plane y=k with 80% opacity.
    ax1.bar(xs, ys, zs=k, zdir='y', color=cs, alpha=0.8)

ax1.set_xlabel(u'Оборот')
ax1.set_ylabel(u'')
ax1.set_zlabel(u'F')

# On the y axis let's only label the discrete values that we have data for.
ax1.set_yticks(yticks, ['u','h','k','m'])

ax2 = fig.add_subplot(224)

#ax2.hist(y1, histtype="stepfilled",
#            bins=25, alpha=0.8, density=True)
#ax2.hist(y2, histtype="stepfilled",
#            bins=25, alpha=0.8, density=True)
#ax2.hist(y3, histtype="stepfilled",
#            bins=25, alpha=0.8, density=True)

dist_names = ['beta'] #['alpha', 'beta', 'arcsine','weibull_min', 'weibull_max', 'rayleigh']

for dist_name in dist_names:
    dist = getattr(sc.stats, dist_name)
    #
    param = dist.fit(y1)
    pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1]) * size
    ax2.plot(pdf_fitted, label=dist_name, color=colors[0])
    #
    param = dist.fit(y2)
    pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1]) * size
    ax2.plot(pdf_fitted, label=dist_name, color=colors[1])
    #
    param = dist.fit(y3)
    pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1]) * size
    ax2.plot(pdf_fitted, label=dist_name, color=colors[2])
    plt.xlim(0,50)

plt.show()



test3
test4
test5
test5
xdata, ydata = np.random.random((2, 10))
tmp_points = []
tmp_colors = []
fig, ax = plt.subplots()
for i,j in zip(xdata,ydata):
    line, = ax.plot(i, j, 'bo')
    tmp_points.append(line)
    tmp_colors.append(line.get_color)

point, = ax.plot([],[], marker="o", color="crimson")
text = ax.text(0,0,"")

def line_select_callback(eclick, erelease):
    x1, y1 = eclick.xdata, eclick.ydata
    x2, y2 = erelease.xdata, erelease.ydata

    mask= (xdata > min(x1,x2)) & (xdata < max(x1,x2)) & \
          (ydata > min(y1,y2)) & (ydata < max(y1,y2))
    xmasked = xdata[mask]
    ymasked = ydata[mask]

    inds = np.nonzero(mask)

    if len(xmasked) > 0:
        #
        # Read artists from scatter and find color of points inside rectangle
        t_i = np.nonzero('b' == tmp_colors[inds])
        if(len(t_i) > 0):
            [i = 'r' for i in tmp_colors[t_i]]
            tmp_points[t_i].set_color ='r'
        t_i = np.nonzero('b' != tmp_colors[inds])
        if(len(t_i) > 0):
            tmp_colors[t_i] = 'b'
            tmp_points[t_i].set_color ='b'
test6

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.widgets  import RectangleSelector

xdata = np.linspace(0,9*np.pi, num=301)
ydata = np.sin(xdata)*np.cos(xdata*2.4)

fig, ax = plt.subplots()
line, = ax.plot(xdata, ydata, 'bo')
point, = ax.plot([],[], marker="o", color="crimson")
text = ax.text(0,0,"")

def line_select_callback(eclick, erelease):
    x1, y1 = eclick.xdata, eclick.ydata
    x2, y2 = erelease.xdata, erelease.ydata

    mask= (xdata > min(x1,x2)) & (xdata < max(x1,x2)) & \
          (ydata > min(y1,y2)) & (ydata < max(y1,y2))
    xmasked = xdata[mask]
    ymasked = ydata[mask]

    if len(xmasked) > 0:
        #for i,j in zip(xmasked,ymasked):
        #    ax.plot(i,j, marker="o", color="crimson")
        print(xmasked,ymasked)
        point.set_data(xmasked,ymasked)
        #xmax = xmasked[np.argmax(ymasked)]
        #ymax = ymasked.max()
        #tx = "xmax: {:.3f}, ymax {:.3f}".format(xmax,ymax)
        #point.set_data([xmax],[ymax])
        #text.set_text(tx)
        #text.set_position((xmax,ymax))
        #

        #
        fig.canvas.draw_idle()


rs = RectangleSelector(ax, line_select_callback,
                       drawtype='box', useblit=False, button=[1],
                       minspanx=5, minspany=5, spancoords='pixels')#,
                       #interactive=True)

plt.show()

https://duckduckgo.com/?q=python+estimate+square+of+histogram&kp=1&t=ffsb&ia=qa
https://nahlogin.blogspot.com/2016/01/pandas.html
https://pythonhosted.org/PyQt-Fit/KDE_tut.html
http://scikit-learn.org/stable/modules/density.html
http://benalexkeen.com/basic-statistics-in-python/

test6
test_ya
test_ya

# -*- coding: utf-8 -*-
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn import model_selection
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import explained_variance_score, mean_squared_error
#
df = pd.read_excel(u'', sheet_name = u'обучение') # контроль
#df.info
#
df.set_index(df.loc[:,'xxx'], inplace=True)

asx1 = df.columns[7:].values.tolist()
asx2 = []
tmp_nam_feat = [x for x in asx1 if x not in asx2]
#
asx1 = df.loc[:, df.columns[0]].values
asx2 = [] # iiiind2_neg #
tmp_nam_samp = [x for x in asx1 if x not in asx2]
#
tmp_X_test = df.loc[tmp_nam_samp,tmp_nam_feat].values
tmp_Y_test = df.loc[tmp_nam_samp,'xxxxx'].values
print(tmp_X_test.shape)
print(tmp_Y_test.shape)
#print(iiiind2_neg[:10])
#print(asx1[:10])
#print(tmp_nam_samp[:10])


X_reg = tmp_X_test
Y_reg = tmp_Y_test
#
reg1 = PLSRegression(n_components=8)
reg2 = DecisionTreeRegressor(max_depth=4)
reg3 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\
                        n_estimators=300) #, random_state=rng
#
IndexesD = range(0,X_reg.shape[0])
kfold = model_selection.KFold(10, True, 1)
out_t = []
for train, test in kfold.split(IndexesD):
    reg1.fit(X_reg[train,:], Y_reg[train])
    reg2.fit(X_reg[train,:], Y_reg[train])
    reg3.fit(X_reg[train,:], Y_reg[train])
    #
    y1_p = reg1.predict(X_reg[test,:])
    y2_p = reg2.predict(X_reg[test,:])
    y3_p = reg3.predict(X_reg[test,:])
    #
    out_t.extend([100*np.mean(y1_p/Y_reg[test]),\
                  100*np.mean(y2_p/Y_reg[test]),\
                  100*np.mean(y3_p/Y_reg[test])])

pl_out_t = np.array(out_t).reshape(int(len(out_t)/3),3)
plt.plot(pl_out_t[:,0], 'r', pl_out_t[:,1], 'g', pl_out_t[:,2], 'b')
plt.xlabel('kfold')
plt.ylabel('$Y_{predccit} / Y$')
plt.legend(['P','T','A'])
plt.show()

pl_out_t = np.array(out_t).reshape(int(len(out_t)/3),3)
plt.plot(pl_out_t[:,0], 'r', pl_out_t[:,1], 'g', pl_out_t[:,2], 'b')
plt.xlabel('kfold')
plt.ylabel('$Y_{predccit} / Y$')
plt.legend(['P','T','A'])
plt.show()

X_scal = tmp_X_test
#
scaler = preprocessing.StandardScaler()
scaler.fit(X_scal)
X_s = scaler.transform(X_scal)
#
fig = plt.figure(figsize=(18,6))
plt.subplot(121)
plt.plot(X_s.transpose().conj())
plt.xticks(range(0,X_s.shape[1]), tmp_nam_feat, rotation=90)
plt.grid(True)
plt.subplot(122)
plt.plot(X_s)
plt.show()

XX = X_s
fig, ax = plt.subplots(num=None, figsize=(16, 18), dpi=80, facecolor='w', edgecolor='k')
ax.stackplot(range(0,XX.shape[0]), XX.transpose().conj())
#plt.xlim(0,100)
plt.show()


fig = plt.figure(figsize=(18,6))

data = X_s.transpose().conj() # tmp_X_test.transpose().conj()

columns = (tmp_nam_samp)

rows = ['%s' % x for x in tmp_nam_feat]

values = np.arange(0, len(tmp_nam_feat), 1)
value_increment = 1

fig, ax = plt.subplots(num=None, figsize=(16, 18), dpi=80, facecolor='w', edgecolor='k')

# Get some pastel shades for the colors
colors = plt.cm.Greens(np.linspace(0, 0.5, len(rows))) #YlGnBu BuPu
n_rows = len(data)

index = np.arange(len(columns)) + 0.3
bar_width = 0.8

# Initialize the vertical-offset for the stacked bar chart.
y_offset = np.zeros(len(columns))

# Plot bars and create text labels for the table
cell_text = []
for row in range(n_rows):
    plt.bar(index, data[row], bar_width, bottom=y_offset, color=colors[row])
    y_offset = y_offset + data[row]
    cell_text.append(['%1.1f' % (x / 1000.0) for x in y_offset])

plt.show()

pca = PCA(n_components=8)
X_tr = pca.fit_transform(X_s)
X_com = pca.components_

tmp_mah = []
for i in X_tr:
    V = np.cov(X_tr.T)
    VI = np.linalg.inv(V)
    tmp_mah.append( np.sqrt( np.max(np.diag((i-np.mean(X_tr, 0)).T * VI * (i-np.mean(X_tr, 0)))) ) ) 
with open('tmp.txt', 'w') as of: of.write( ','.join([str(i) for i in tmp_mah]) )
with open('tmp.txt', 'r') as of: tmp_in = of.read(); tmp_in = [np.float(i) for i in tmp_in.split(',')]

fig = plt.figure(figsize=(20,12))
ax = fig.add_subplot(2,1,2)
plt.hist(tmp_in)
ax = fig.add_subplot(2,2,1)
plt.plot(X_com[0,:],X_com[1,:],'bo')
plt.xlabel('PC1'); plt.ylabel('PC2');plt.grid()
[plt.text(x, y, l) for x,y,l in zip(X_com[0,:],X_com[1,:],tmp_nam_feat)]
ax = fig.add_subplot(2,2,2)
plt.plot(X_tr[:,0],X_tr[:,1],'bo')
plt.xlabel('PC1'); plt.ylabel('PC2');plt.grid()

plt.show()


X_pca = np.dot(X_tr, X_com)
E = X_s - X_pca

fig = plt.figure(figsize=(20,10))
fig.subplots_adjust(hspace=0.8)

ax = fig.add_subplot(212)
tmp_mean_samp = np.mean(E, 1)
plt.hist(tmp_mean_samp)

ax = fig.add_subplot(221)
plt.plot(np.mean(E,0))
plt.xticks(range(0,E.shape[1]), tmp_nam_feat, rotation='vertical')
plt.xlabel('Variables')
plt.ylabel('Weight')
plt.title('Weights Variables')
plt.grid()

ax = fig.add_subplot(222)
plt.plot(tmp_in, np.abs(np.mean(E,1)), 'go')
[plt.text(x, y, l) for x,y,l in zip(tmp_in,np.abs(np.mean(E,1)),tmp_nam_samp)]
plt.xlabel('Leverage')
plt.ylabel('Residual')
plt.title('Influence Samples')

plt.show()


ttt = df.set_index('xxx')

mass = np.asarray(ro_arr)
iiiind1_pos = [ tmp_nam_samp[i] for i in (mass < 1.0).nonzero()[0] ]
iiiind2_pos = [ tmp_nam_samp[i] for i in ((mass > 1) & (mass < 2)).nonzero()[0] ]
iiiind3_pos = [ tmp_nam_samp[i] for i in (mass > 2).nonzero()[0] ]

mass = tmp_mean_samp
iiiind1_neg = [ tmp_nam_samp[i] for i in ((mass < -0.15) | (mass > 0.15)).nonzero()[0] ]
iiiind2_neg = [ tmp_nam_samp[i] for i in ((mass > -0.1) & (mass < 0.1)).nonzero()[0] ]

param = 'xxxxx' # 

fig = plt.figure(num=None, figsize=(16, 3), dpi=80, facecolor='w', edgecolor='k')
plt.plot( ( ttt.loc[iiiind1_pos, param] ).values, 'r',\
         (ttt.loc[list( set(iiiind2_pos) - set(iiiind2_neg) ), param] ).values, 'b',\
         ( ttt.loc[iiiind3_pos, param] ).values, 'g')
plt.show()


plt.style.use('bmh')
fig = plt.figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')

#ax = plt.subplots()

plt.hist(( ttt.loc[iiiind1_pos, param] ).values, histtype="stepfilled",
            alpha=0.8, density=False, color='r')
plt.hist((ttt.loc[list( set(iiiind2_pos)  ), param] ).values, histtype="stepfilled",
           alpha=0.8, density=False, color='b')
plt.hist(( ttt.loc[iiiind3_pos, param] ).values, histtype="stepfilled",
            alpha=0.8, density=False, color='g')
#- set(iiiind2_neg)
plt.show()

def show_arr1(df_in, show_arr_in):
    plt.figure(num=None, figsize=(20, 18), dpi=80, facecolor='w', edgecolor='k')
    n = len(show_arr_in)
    n1 = n//3 + 1
    n2 = 3
    print(n,n1,n2)
    count = 1
    for ind in show_arr_in:
        ax = plt.subplot(n1, n2, count)
        plt.hist(df[ind])
        count = count + 1
    plt.show()

show_arr1(df, df.columns[7:])








popsize
Размер популяции.
mutprob
Вероятность того, что новая особь будет результатом мутации, а не
скрещивания.
elite
Доля особей в популяции, считающихся хорошими решениями и пе-
реходящих в следующее поколение.
maxiter
Количество поколений.
Попробуем оптимизировать план путешествия с помощью генетичес-
кого алгоритма:
>>> s=optimization.geneticoptimize(domain,optimization.schedulecost)
3532
3503
...
2591
2591
2591
>>> optimization.printschedule(s)
