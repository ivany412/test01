

===============================================================================
===============================================================================
===============================================================================
# -*- coding: utf-8 -*-
#https://stackoverflow.com/questions/42172926/pool-map-list-index-out-of-range-python
#http://qaru.site/questions/1139485/python-multiprocessing-poolmap-raises-indexerror
#http://qaru.site/questions/1139487/debugging-errors-in-python-multiprocessing
import time
import pandas as pd
import multiprocessing as mp,os
import pickle

count = 0

def t_process(line):
    global count
    t_line = line[:-1].split(';')
    print(t_line[-1])
    if (t_line[-1]==1):
        count = count + 1
        print(count)

def process_wrapper(chunkStart, chunkSize, item):
    with open("") as f:
        f.seek(chunkStart)
        lines = f.read(chunkSize).splitlines()
        out = []
        for line in lines:
            t_l = line[:-1].split(';')
            if (t_l[1] == item):
                out.append( [int(t_l[2]), float(t_l[6].replace(',','.'))] )
    return out

def chunkify(fname,size=1024*1024*128):
    fileEnd = os.path.getsize(fname)
    with open(fname,'r') as f:
        chunkEnd = f.tell()
        while True:
            chunkStart = chunkEnd
            f.seek(size, 1)
            f.readline()
            chunkEnd = f.tell()
            yield chunkStart, chunkEnd - chunkStart
            if chunkEnd > fileEnd:
                break

if __name__ == '__main__':
    cores = 2
    #init objects
    #pool = mp.Pool(cores)
    #jobs = []

    with open("") as of:
        tmp_r = of.read().split('\n')
    struc = {}
    for item1 in tmp_r:
        print(item1)
        pool = mp.Pool(cores)
        jobs = []
        #create jobs
        for chunkStart,chunkSize in chunkify(""):
            #print(chunkStart,chunkSize)
            jobs.append( pool.apply_async(process_wrapper,(chunkStart,chunkSize,item1)) )
        print(len(jobs))
        #clean up
        pool.close()
        pool.join()
        #wait for all jobs to finish
        tmp = []
        for job in jobs:
            #time.sleep(20)
            tmp.extend( job.get())
            #tmp = pd.Series(tmp).unique().tolist()
            print(len(tmp))
        print('' + str(item1) + ' done!')
        #
        tmp_m = pd.DataFrame(tmp)
        struc[int(item1)] = {}
        for ind1 in tmp_m[0].unique():
            struc[int(item1)][ind1] = sum(tmp_m.loc[tmp_m.index[tmp_m[0]==ind1], 1])
        with open("" + "_" + str(item1) + ".txt", "w") as of:
            of.writelines([str((i,struc[int(item1)][i]))[1:-1] +\
             '\n' for i in struc[int(item1)].keys()])
    #
    file = open('filename_save.pkl', 'w')
    pickle.dump(struc, file)
    print('all done')
===============================================================================
===============================================================================
===============================================================================
# -*- coding: utf-8 -*-
import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#
def fun_proc(data):
    bs = np.percentile(data, [25,50,75], axis=0)
    h = plt.hist(data)
    inds = (h[0] > bs[-1]).nonzero()
    if (np.max(h[0][inds]) - np.min(h[0][inds]) != 0):
        return (np.max(h[1][inds]) - np.min(h[1][inds]))/\
            (np.max(h[0][inds]) - np.min(h[0][inds]))
    else: return 0

def main():
    if (len(sys.argv[1]) == 0):
        print("Error: No input a file")
    else:
        pathdir = sys.argv[1]
        out = []
        for item1 in os.listdir(pathdir):
            data = pd.read_csv(pathdir + '\\' + item1, sep=",", header=None)
            out.append( [int(item1.split("_")[-1][:-4]),fun_proc(data[1])] )
        #
        y = np.array(out)
        bs = np.percentile(y[:,1], [25,50,75], axis=0)
        inds1 = ((y[:,1] < bs[0]) | (y[:,1] == bs[0])).nonzero()[0]
        inds2 = ((y[:,1] > bs[0]) & (y[:,1] < bs[1]) | (y[:,1] == bs[1])).nonzero()[0]
        inds3 = ((y[:,1] > bs[1]) & (y[:,1] < bs[2])).nonzero()[0]
        inds4 = ((y[:,1] > bs[2]) | (y[:,1] == bs[2])).nonzero()[0]
        # add label
        labels = np.ones((len(y[:,1]),1))
        print(labels.shape)
        if (len(inds1) != 0):
            labels[inds1] = 0
        if (len(inds2) != 0):
            labels[inds2] = 1
        if (len(inds3) != 0):
            labels[inds3] = 2
        if (len(inds4) != 0):
            labels[inds4] = 3
        y = np.hstack((y,labels))
        # save result to file
        with open("out_file.txt", "w") as of:
            for i in y:
                of.writelines(str(i)[1:-1] + '\n')

if __name__ == '__main__':
    main()
===============================================================================
===============================================================================
===============================================================================

#-------------------------------------------------------------------------------
# Name:        module1
# Purpose:
#
# Author:      admin2
#
# Created:     12.11.2018
# Copyright:   (c) admin2 2018
# Licence:     <your licence>
#-------------------------------------------------------------------------------
import numpy as np
import multiprocessing as mp,os

PathFile = "input.txt"

def process_wrapper(chunkStart, chunkSize):
    with open(PathFile) as f:
        f.seek(chunkStart)
        lines = f.read(chunkSize).splitlines()
        for line in lines:
            t_l = line[-1].split(';')
            struc = fun_pars_ch(t_l[1], t_l[2], t_l[5], struc)
    return struc

def chunkify(fname,size=1024*1024*128):
    fileEnd = os.path.getsize(fname)
    with open(fname,'r') as f:
        chunkEnd = f.tell()
        while True:
            chunkStart = chunkEnd
            f.seek(size,1)
            f.readline()
            chunkEnd = f.tell()
            yield chunkStart, chunkEnd - chunkStart
            if chunkEnd > fileEnd:
                break

def fun_pars_ch(a, b, c, st):
    t1_ind = (np.array([int(i) for i in st.keys()]) == a).nonzero()[0]
    if (len(t1_ind) == 1):
        t2_ind = (np.array([int(i) for i in st[str(a)].keys()]) == b).nonzero()[0]
        if (len(t2_ind) == 1):
            st[str(a)][str(b)] = st[str(a)][str(b)] + c
        else:
            st[str(a)][str(b)] = c
    else:
        st[str(a)] = {str(b): c}
    return st

def main():
    cores = 4
    #init objects
    pool = mp.Pool(cores)
    jobs = []

    #create jobs
    for chunkStart,chunkSize in chunkify(PathFile):
        jobs.append( pool.apply_async(process_wrapper,(chunkStart,chunkSize)) )
    tmp = []
    #wait for all jobs to finish
    for job in jobs:
        tmp.extend(job.get())

    #clean up
    pool.close()

    struc_out = {}
    for i in tmp:
        for j in i.keys():
            for k in j[i].keys():
                struc_out = fun_pars_ch(i[j][k], i[j][k], i[a.keys()[0]], struc_out)
    #
    #print(struc)


if __name__ == '__main__':
    main()

test




#-------------------------------------------------------------------------------
# Name:        module1
# Purpose:
#
# Author:      admin2
#
# Created:     13.11.2018
# Copyright:   (c) admin2 2018
# Licence:     <your licence>
#-------------------------------------------------------------------------------
#https://stackoverflow.com/questions/27731458/fastest-way-to-write-large-csv-with-python
import random
import uuid
import numpy as np
import csv
import os

def main1():
    outfile = 'data.csv'
    outsize = 1024 * 1024 * 1024 # 1GB
    with open(outfile, 'ab') as csvfile:
        size = 0
        while size < outsize:
            txt = '%s,%.6f,%.6f,%i\n' % (uuid.uuid4(), random.random()*50, random.random()*50, random.randrange(1000))
            size += len(txt)
            csvfile.write(txt)

def main2():
    outfile = 'data-alt.csv'
    outsize = 1000 # MB
    chunksize = 1000
    with open(outfile, 'ab') as csvfile:
        while (os.path.getsize(outfile)//1024**2) < outsize:
            data = [[uuid.uuid4() for i in range(chunksize)],
                    np.random.random(chunksize)*50,
                    np.random.random(chunksize)*50,
                    np.random.randint(1000, size=(chunksize,))]
            csvfile.writelines(['%s,%.6f,%.6f,%i\n' % row for row in zip(*data)])

if __name__ == '__main__':
    main2()


test2


# -*- coding: utf-8 -*-
# This import registers the 3D projection, but is otherwise unused.
from mpl_toolkits.mplot3d import Axes3D 
from numpy.random import beta
import matplotlib.pyplot as plt
import scipy as sc

size=10000
x = scipy.arange(size)
y1 = beta(10, 10, size=size)*25
y2 = beta(4, 12, size=size)*25
y3 = beta(50, 12, size=size)*25


h = [h1, h2, h3]

fig = plt.figure(figsize=(20,10))

ax0 = fig.add_subplot(221)

h1 = plt.hist(y1, histtype="stepfilled",
            bins=25, alpha=0.8, density=True)
h2 = plt.hist(y2, histtype="stepfilled",
            bins=25, alpha=0.8, density=True)
h3 = plt.hist(y3, histtype="stepfilled",
            bins=25, alpha=0.8, density=True)

ax1 = fig.add_subplot(223, projection='3d')

y = []
colors = ['r', 'g', 'b']
yticks = [3, 2, 1]
for c, k, s in zip(colors, yticks, h):
    # Generate the random data for the y=k 'layer'.
    ys = s[0]
    xs = np.arange(len(ys))
    # You can provide either a single color or an array with the same length as
    # xs and ys. To demonstrate this, we color the first bar of each set cyan.
    cs = [c] * len(xs)
    #cs[0] = 'c'

    # Plot the bar graph given by xs and ys on the plane y=k with 80% opacity.
    ax1.bar(xs, ys, zs=k, zdir='y', color=cs, alpha=0.8)

ax1.set_xlabel(u'Оборот')
ax1.set_ylabel(u'')
ax1.set_zlabel(u'F')

# On the y axis let's only label the discrete values that we have data for.
ax1.set_yticks(yticks, ['u','h','k','m'])

ax2 = fig.add_subplot(224)

#ax2.hist(y1, histtype="stepfilled",
#            bins=25, alpha=0.8, density=True)
#ax2.hist(y2, histtype="stepfilled",
#            bins=25, alpha=0.8, density=True)
#ax2.hist(y3, histtype="stepfilled",
#            bins=25, alpha=0.8, density=True)

dist_names = ['beta'] #['alpha', 'beta', 'arcsine','weibull_min', 'weibull_max', 'rayleigh']

for dist_name in dist_names:
    dist = getattr(sc.stats, dist_name)
    #
    param = dist.fit(y1)
    pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1]) * size
    ax2.plot(pdf_fitted, label=dist_name, color=colors[0])
    #
    param = dist.fit(y2)
    pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1]) * size
    ax2.plot(pdf_fitted, label=dist_name, color=colors[1])
    #
    param = dist.fit(y3)
    pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1]) * size
    ax2.plot(pdf_fitted, label=dist_name, color=colors[2])
    plt.xlim(0,50)

plt.show()



test3
test4
test5
test5
xdata, ydata = np.random.random((2, 10))
tmp_points = []
tmp_colors = []
fig, ax = plt.subplots()
for i,j in zip(xdata,ydata):
    line, = ax.plot(i, j, 'bo')
    tmp_points.append(line)
    tmp_colors.append(line.get_color)

point, = ax.plot([],[], marker="o", color="crimson")
text = ax.text(0,0,"")

def line_select_callback(eclick, erelease):
    x1, y1 = eclick.xdata, eclick.ydata
    x2, y2 = erelease.xdata, erelease.ydata

    mask= (xdata > min(x1,x2)) & (xdata < max(x1,x2)) & \
          (ydata > min(y1,y2)) & (ydata < max(y1,y2))
    xmasked = xdata[mask]
    ymasked = ydata[mask]

    inds = np.nonzero(mask)

    if len(xmasked) > 0:
        #
        # Read artists from scatter and find color of points inside rectangle
        t_i = np.nonzero('b' == tmp_colors[inds])
        if(len(t_i) > 0):
            [i = 'r' for i in tmp_colors[t_i]]
            tmp_points[t_i].set_color ='r'
        t_i = np.nonzero('b' != tmp_colors[inds])
        if(len(t_i) > 0):
            tmp_colors[t_i] = 'b'
            tmp_points[t_i].set_color ='b'
test6

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.widgets  import RectangleSelector

xdata = np.linspace(0,9*np.pi, num=301)
ydata = np.sin(xdata)*np.cos(xdata*2.4)

fig, ax = plt.subplots()
line, = ax.plot(xdata, ydata, 'bo')
point, = ax.plot([],[], marker="o", color="crimson")
text = ax.text(0,0,"")

def line_select_callback(eclick, erelease):
    x1, y1 = eclick.xdata, eclick.ydata
    x2, y2 = erelease.xdata, erelease.ydata

    mask= (xdata > min(x1,x2)) & (xdata < max(x1,x2)) & \
          (ydata > min(y1,y2)) & (ydata < max(y1,y2))
    xmasked = xdata[mask]
    ymasked = ydata[mask]

    if len(xmasked) > 0:
        #for i,j in zip(xmasked,ymasked):
        #    ax.plot(i,j, marker="o", color="crimson")
        print(xmasked,ymasked)
        point.set_data(xmasked,ymasked)
        #xmax = xmasked[np.argmax(ymasked)]
        #ymax = ymasked.max()
        #tx = "xmax: {:.3f}, ymax {:.3f}".format(xmax,ymax)
        #point.set_data([xmax],[ymax])
        #text.set_text(tx)
        #text.set_position((xmax,ymax))
        #

        #
        fig.canvas.draw_idle()


rs = RectangleSelector(ax, line_select_callback,
                       drawtype='box', useblit=False, button=[1],
                       minspanx=5, minspany=5, spancoords='pixels')#,
                       #interactive=True)

plt.show()

https://duckduckgo.com/?q=python+estimate+square+of+histogram&kp=1&t=ffsb&ia=qa
https://nahlogin.blogspot.com/2016/01/pandas.html
https://pythonhosted.org/PyQt-Fit/KDE_tut.html
http://scikit-learn.org/stable/modules/density.html
http://benalexkeen.com/basic-statistics-in-python/

test6
test_ya
test_ya

# -*- coding: utf-8 -*-
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn import model_selection
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import explained_variance_score, mean_squared_error
#
df = pd.read_excel(u'', sheet_name = u'обучение') # контроль
#df.info
#
df.set_index(df.loc[:,'xxx'], inplace=True)

asx1 = df.columns[7:].values.tolist()
asx2 = []
tmp_nam_feat = [x for x in asx1 if x not in asx2]
#
asx1 = df.loc[:, df.columns[0]].values
asx2 = [] # iiiind2_neg #
tmp_nam_samp = [x for x in asx1 if x not in asx2]
#
tmp_X_test = df.loc[tmp_nam_samp,tmp_nam_feat].values
tmp_Y_test = df.loc[tmp_nam_samp,'xxxxx'].values
print(tmp_X_test.shape)
print(tmp_Y_test.shape)
#print(iiiind2_neg[:10])
#print(asx1[:10])
#print(tmp_nam_samp[:10])


X_reg = tmp_X_test
Y_reg = tmp_Y_test
#
reg1 = PLSRegression(n_components=8)
reg2 = DecisionTreeRegressor(max_depth=4)
reg3 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\
                        n_estimators=300) #, random_state=rng
#
IndexesD = range(0,X_reg.shape[0])
kfold = model_selection.KFold(10, True, 1)
out_t = []
for train, test in kfold.split(IndexesD):
    reg1.fit(X_reg[train,:], Y_reg[train])
    reg2.fit(X_reg[train,:], Y_reg[train])
    reg3.fit(X_reg[train,:], Y_reg[train])
    #
    y1_p = reg1.predict(X_reg[test,:])
    y2_p = reg2.predict(X_reg[test,:])
    y3_p = reg3.predict(X_reg[test,:])
    #
    out_t.extend([100*np.mean(y1_p/Y_reg[test]),\
                  100*np.mean(y2_p/Y_reg[test]),\
                  100*np.mean(y3_p/Y_reg[test])])

pl_out_t = np.array(out_t).reshape(int(len(out_t)/3),3)
plt.plot(pl_out_t[:,0], 'r', pl_out_t[:,1], 'g', pl_out_t[:,2], 'b')
plt.xlabel('kfold')
plt.ylabel('$Y_{predccit} / Y$')
plt.legend(['P','T','A'])
plt.show()

pl_out_t = np.array(out_t).reshape(int(len(out_t)/3),3)
plt.plot(pl_out_t[:,0], 'r', pl_out_t[:,1], 'g', pl_out_t[:,2], 'b')
plt.xlabel('kfold')
plt.ylabel('$Y_{predccit} / Y$')
plt.legend(['P','T','A'])
plt.show()

X_scal = tmp_X_test
#
scaler = preprocessing.StandardScaler()
scaler.fit(X_scal)
X_s = scaler.transform(X_scal)
#
fig = plt.figure(figsize=(18,6))
plt.subplot(121)
plt.plot(X_s.transpose().conj())
plt.xticks(range(0,X_s.shape[1]), tmp_nam_feat, rotation=90)
plt.grid(True)
plt.subplot(122)
plt.plot(X_s)
plt.show()

XX = X_s
fig, ax = plt.subplots(num=None, figsize=(16, 18), dpi=80, facecolor='w', edgecolor='k')
ax.stackplot(range(0,XX.shape[0]), XX.transpose().conj())
#plt.xlim(0,100)
plt.show()


fig = plt.figure(figsize=(18,6))

data = X_s.transpose().conj() # tmp_X_test.transpose().conj()

columns = (tmp_nam_samp)

rows = ['%s' % x for x in tmp_nam_feat]

values = np.arange(0, len(tmp_nam_feat), 1)
value_increment = 1

fig, ax = plt.subplots(num=None, figsize=(16, 18), dpi=80, facecolor='w', edgecolor='k')

# Get some pastel shades for the colors
colors = plt.cm.Greens(np.linspace(0, 0.5, len(rows))) #YlGnBu BuPu
n_rows = len(data)

index = np.arange(len(columns)) + 0.3
bar_width = 0.8

# Initialize the vertical-offset for the stacked bar chart.
y_offset = np.zeros(len(columns))

# Plot bars and create text labels for the table
cell_text = []
for row in range(n_rows):
    plt.bar(index, data[row], bar_width, bottom=y_offset, color=colors[row])
    y_offset = y_offset + data[row]
    cell_text.append(['%1.1f' % (x / 1000.0) for x in y_offset])

plt.show()

pca = PCA(n_components=8)
X_tr = pca.fit_transform(X_s)
X_com = pca.components_

tmp_mah = []
for i in X_tr:
    V = np.cov(X_tr.T)
    VI = np.linalg.inv(V)
    tmp_mah.append( np.sqrt( np.max(np.diag((i-np.mean(X_tr, 0)).T * VI * (i-np.mean(X_tr, 0)))) ) ) 
with open('tmp.txt', 'w') as of: of.write( ','.join([str(i) for i in tmp_mah]) )
with open('tmp.txt', 'r') as of: tmp_in = of.read(); tmp_in = [np.float(i) for i in tmp_in.split(',')]

fig = plt.figure(figsize=(20,12))
ax = fig.add_subplot(2,1,2)
plt.hist(tmp_in)
ax = fig.add_subplot(2,2,1)
plt.plot(X_com[0,:],X_com[1,:],'bo')
plt.xlabel('PC1'); plt.ylabel('PC2');plt.grid()
[plt.text(x, y, l) for x,y,l in zip(X_com[0,:],X_com[1,:],tmp_nam_feat)]
ax = fig.add_subplot(2,2,2)
plt.plot(X_tr[:,0],X_tr[:,1],'bo')
plt.xlabel('PC1'); plt.ylabel('PC2');plt.grid()

plt.show()


X_pca = np.dot(X_tr, X_com)
E = X_s - X_pca

fig = plt.figure(figsize=(20,10))
fig.subplots_adjust(hspace=0.8)

ax = fig.add_subplot(212)
tmp_mean_samp = np.mean(E, 1)
plt.hist(tmp_mean_samp)

ax = fig.add_subplot(221)
plt.plot(np.mean(E,0))
plt.xticks(range(0,E.shape[1]), tmp_nam_feat, rotation='vertical')
plt.xlabel('Variables')
plt.ylabel('Weight')
plt.title('Weights Variables')
plt.grid()

ax = fig.add_subplot(222)
plt.plot(tmp_in, np.abs(np.mean(E,1)), 'go')
[plt.text(x, y, l) for x,y,l in zip(tmp_in,np.abs(np.mean(E,1)),tmp_nam_samp)]
plt.xlabel('Leverage')
plt.ylabel('Residual')
plt.title('Influence Samples')

plt.show()


ttt = df.set_index('xxx')

mass = np.asarray(ro_arr)
iiiind1_pos = [ tmp_nam_samp[i] for i in (mass < 1.0).nonzero()[0] ]
iiiind2_pos = [ tmp_nam_samp[i] for i in ((mass > 1) & (mass < 2)).nonzero()[0] ]
iiiind3_pos = [ tmp_nam_samp[i] for i in (mass > 2).nonzero()[0] ]

mass = tmp_mean_samp
iiiind1_neg = [ tmp_nam_samp[i] for i in ((mass < -0.15) | (mass > 0.15)).nonzero()[0] ]
iiiind2_neg = [ tmp_nam_samp[i] for i in ((mass > -0.1) & (mass < 0.1)).nonzero()[0] ]

param = 'xxxxx' # 

fig = plt.figure(num=None, figsize=(16, 3), dpi=80, facecolor='w', edgecolor='k')
plt.plot( ( ttt.loc[iiiind1_pos, param] ).values, 'r',\
         (ttt.loc[list( set(iiiind2_pos) - set(iiiind2_neg) ), param] ).values, 'b',\
         ( ttt.loc[iiiind3_pos, param] ).values, 'g')
plt.show()


plt.style.use('bmh')
fig = plt.figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')

#ax = plt.subplots()

plt.hist(( ttt.loc[iiiind1_pos, param] ).values, histtype="stepfilled",
            alpha=0.8, density=False, color='r')
plt.hist((ttt.loc[list( set(iiiind2_pos)  ), param] ).values, histtype="stepfilled",
           alpha=0.8, density=False, color='b')
plt.hist(( ttt.loc[iiiind3_pos, param] ).values, histtype="stepfilled",
            alpha=0.8, density=False, color='g')
#- set(iiiind2_neg)
plt.show()

def show_arr1(df_in, show_arr_in):
    plt.figure(num=None, figsize=(20, 18), dpi=80, facecolor='w', edgecolor='k')
    n = len(show_arr_in)
    n1 = n//3 + 1
    n2 = 3
    print(n,n1,n2)
    count = 1
    for ind in show_arr_in:
        ax = plt.subplot(n1, n2, count)
        plt.hist(df[ind])
        count = count + 1
    plt.show()

show_arr1(df, df.columns[7:])
